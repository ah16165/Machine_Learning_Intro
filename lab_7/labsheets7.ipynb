{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Principal Component Analysis (PCA)\n",
    "\n",
    "In this lab session we will implement the Principal Component Analysis (PCA) to reduce the dimensionality of the Iris flower dataset.\n",
    "\n",
    "We will first code this ourselves using NumPy and will then validate our solution comparing our results with Scipy's PCA implementation.\n",
    "\n",
    "Finally, we will compare the PCA-reduced samples to the reduced samples we obtained by manually selecting two features in lab 5.\n",
    "\n",
    "As usual, let's import the libraries before we start by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # to avoid issues between Python 2 and 3 printing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# show matplotlib figures inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default we set figures to be 12\"x8\" on a 110 dots per inch (DPI) screen \n",
    "# (adjust DPI if you have a high res screen!)\n",
    "plt.rc('figure', figsize=(12, 8), dpi=110)\n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "In this lab we will use the Iris dataset again. Let's run the cell below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris train and test sets\n",
    "\n",
    "def load_iris_data(train_path='iris_train.csv', test_path='iris_test.csv'):\n",
    "    train_set = np.loadtxt(train_path, delimiter=',')\n",
    "    test_set = np.loadtxt(test_path, delimiter=',')\n",
    "\n",
    "    # separate labels from features\n",
    "    train_labels = train_set[:, 4].astype(np.int)\n",
    "    train_set = train_set[:, 0:4]\n",
    "    test_labels = test_set[:, 4].astype(np.int)\n",
    "    test_set = test_set[:, 0:4]\n",
    "    \n",
    "    return train_labels, train_set, test_labels, test_set\n",
    "\n",
    "train_labels, train_set, test_labels, test_set = load_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA steps\n",
    "\n",
    "To implement PCA we will need to implement the following steps __on the training set__ which is represented as a $n \\times d$ matrix :\n",
    "\n",
    "1. Calculate the covariance matrix \n",
    "2. Calculate the eigenvectors and the corresponding eigenvalues. \n",
    "3. Sort the eigenvectors by decreasing eigenvalues and choose the first $k$ eigenvectors using such order. This is to create a $d \\times k$ matrix $W$ that will be used to project the data into a new lower dimensional space.\n",
    "4. Use the $W$ matrix to transform the samples onto the new space.\n",
    "\n",
    "## 1. Calculate the covariance matrix\n",
    "\n",
    "You can use NumPy's function [`np.cov`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) for this. Pay attention to the `rowvar` argument and make sure your covariance matrix is 4x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate eigenvectors and eigenvalues\n",
    "\n",
    "You can use NumPy's function [`np.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html?highlight=eig#numpy.linalg.eig) for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sort the eigenvectors by decreasing eigenvalues\n",
    "\n",
    "The eigenvalues provide a measure of how much information each eigvenvector carries. By sorting the eigenvectors in descending order according to the respective eigenvalues, we thus sort the various components in decreasing order of importance.\n",
    "\n",
    "There is no guarantee `np.linalg.eig` returns the eigenvectors in a sorted order, so we need to to sort them before creating the $W$ matrix.\n",
    "\n",
    "Choose the first $k$ eigenvectors using such order to create the $d \\times k$ matrix $W$. Print the sorted eigenvectors and the corresponding eigenvalues to check your code. \n",
    "\n",
    "We will use $k=2$, so you should obtain a matrix of shape $4 \\times 2$.\n",
    "\n",
    "__Hint__. There are different ways to achieve this. As a suggestion, our solution used NumPy's functions [`np.flip`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html) and [`np.argsort`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the $W$ matrix to transform the samples onto the new space.\n",
    "\n",
    "Transform __both__ the training and test set. Remember that you can use the `dot` function defined for NumPy arrays to perform matrix multiplication on two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use SciPy's PCA\n",
    "\n",
    "We will now validate our solution comparing it with SciPy's implementation. SciPy provides a class called [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), which we already imported in this labsheet.\n",
    "\n",
    "Steps to follow to use Scipy's PCA:\n",
    "\n",
    "1. Create the PCA object, specifying the number of components (in our case 2). \n",
    "2. Using the created object, [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit) the PCA model on the training set.\n",
    "3. [`Transform`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform) both the training and test sets using the fitted model.\n",
    "\n",
    "Once you've completed the steps above, produce two scatter plots comparing the training set reduced with your PCA to that reduced with Scipy's PCA. You should obtain a plot similar to the one below (we are providing the colour codes we used for the plot). \n",
    "\n",
    "Most importantly, __your scatter plots should be identical__ (axis scale aside) to prove that your implementation is correct.\n",
    "\n",
    "\n",
    "![](pca.png)\n",
    "\n",
    "### Note\n",
    "\n",
    "Scipy's transformed data will be flipped horizontally. This is due to the fact that the eigenvectors can have either a positive or a negative sign, depending on how they are calculated. \n",
    "\n",
    "To obtain the above plot and thus have two identical scatter plots, we flipped Scipy's reduced data along the y axis (tip: you simple need to multiple the second column by `-1` to do that). Note that this does not tamper the data, since what matters is how the samples are located in relationship one with the other. By flipping the samples we are preserving the distribution of the data.\n",
    "\n",
    "Note that the y axis scale of the two plots is also different. This is because Scipy scales the data to have unit length 1. Again, what matters is how the data is distributed, so we can safely tell that our implementation is correct!\n",
    "\n",
    "To obtain the same scaling as our plots, use `ax.set_aspect('equal')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_1_C = r'#3366ff'\n",
    "CLASS_2_C = r'#cc3300'\n",
    "CLASS_3_C = r'#ffc34d'\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with your selected features\n",
    "\n",
    "### Scatter plots\n",
    "\n",
    "Let's finally consider the two features you selected in lab 5. We want to compare the manually reduced dataset to the PCA-reduced one (use the dataset reduced with your own implementation).\n",
    "\n",
    "To do this, we will first compare the two scatter plots. You should obtain something like this:\n",
    "\n",
    "![](pca_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest-Centroid accuracy\n",
    "\n",
    "By looking at the above plots, it seems like our manually reduced dataset separates the data in a better way. \n",
    "\n",
    "Let's prove this by running the Nearest-Centroid classifier you implemented in lab 5, and let's calculate the accuracy as we did in lab 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Using features (3, 4) and our PCA-reduced dataset, we obtained the following results\n",
    "\n",
    "```\n",
    "Accuracy with manually selected features: 0.96\n",
    "Accuracy with PCA: 0.88\n",
    "```\n",
    "\n",
    "In fact, as suspected before, the PCA results are slightly worse. This should not come to a big surprise.\n",
    "\n",
    "PCA excels when dealing with high dimensionality data (e.g. 1024 or more dimensions). In our case, our simple Iris dataset contains 4 features only, and we already observed that, due to the simplicity of the dataset, by carefully selecting 2 features we can obtain nearly perfect results (i.e. 96% of test samples are correctly classified).\n",
    "\n",
    "However, this is an ideal scenario. In real problems you will deal with far more complex and higher dimensional datasets, in which cases PCA will be helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
